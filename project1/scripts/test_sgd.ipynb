{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/49): loss=2259.930562133167, w0=6.595393608119329, w1=7.1239545253972905\n",
      "Stochastic Gradient Descent(1/49): loss=2040.6386141407113, w0=10.575508709016798, w1=2.667400631257233\n",
      "Stochastic Gradient Descent(2/49): loss=1889.5920306685412, w0=13.767748944063484, w1=-0.8397500780906797\n",
      "Stochastic Gradient Descent(3/49): loss=1682.5753166778545, w0=17.663164739260505, w1=-1.9992314560038305\n",
      "Stochastic Gradient Descent(4/49): loss=1586.9811611006453, w0=20.167599288996986, w1=-4.430742094316385\n",
      "Stochastic Gradient Descent(5/49): loss=1175.3100134243546, w0=25.941532377764265, w1=4.670657549494199\n",
      "Stochastic Gradient Descent(6/49): loss=976.4049292384602, w0=29.73352933801274, w1=8.526907806362663\n",
      "Stochastic Gradient Descent(7/49): loss=858.0634436970029, w0=32.69239036524713, w1=7.407584274487076\n",
      "Stochastic Gradient Descent(8/49): loss=747.6582261927867, w0=35.456897910944335, w1=7.743486796746756\n",
      "Stochastic Gradient Descent(9/49): loss=692.0753698440682, w0=37.54906428933259, w1=4.780050887312648\n",
      "Stochastic Gradient Descent(10/49): loss=570.4361359529481, w0=40.784731869592086, w1=6.1822434836093425\n",
      "Stochastic Gradient Descent(11/49): loss=474.8060178176083, w0=43.47308582433037, w1=8.042986438962169\n",
      "Stochastic Gradient Descent(12/49): loss=436.41056686105514, w0=45.09730449861055, w1=6.624049130386678\n",
      "Stochastic Gradient Descent(13/49): loss=402.94538650461476, w0=46.51308699999409, w1=5.870121522116532\n",
      "Stochastic Gradient Descent(14/49): loss=375.5651288355332, w0=47.78717942358803, w1=5.127193954514578\n",
      "Stochastic Gradient Descent(15/49): loss=354.2041484934348, w0=49.302638744813294, w1=3.377492435523729\n",
      "Stochastic Gradient Descent(16/49): loss=287.8486460568923, w0=51.45292685611492, w1=5.239782374062829\n",
      "Stochastic Gradient Descent(17/49): loss=198.64838449625765, w0=54.2892292667867, w1=11.167430598579562\n",
      "Stochastic Gradient Descent(18/49): loss=182.44015813794314, w0=55.44228836975059, w1=9.55189915093216\n",
      "Stochastic Gradient Descent(19/49): loss=172.26265393658005, w0=56.03077643978516, w1=9.51268112308191\n",
      "Stochastic Gradient Descent(20/49): loss=138.45663674168256, w0=57.91384595428739, w1=10.38217171547501\n",
      "Stochastic Gradient Descent(21/49): loss=133.98730738347908, w0=58.30719805809629, w1=9.92993153254311\n",
      "Stochastic Gradient Descent(22/49): loss=124.2544573689499, w0=59.30876134332447, w1=8.77307665682083\n",
      "Stochastic Gradient Descent(23/49): loss=115.19853800768063, w0=60.439688835855044, w1=7.615073487417139\n",
      "Stochastic Gradient Descent(24/49): loss=88.98644229300282, w0=62.07505251129404, w1=8.86039688000759\n",
      "Stochastic Gradient Descent(25/49): loss=71.86913184095756, w0=63.101280477267494, w1=10.46698166533659\n",
      "Stochastic Gradient Descent(26/49): loss=60.41900039662701, w0=64.3438626595897, w1=10.32334382200251\n",
      "Stochastic Gradient Descent(27/49): loss=48.64621373524543, w0=65.46305724144256, w1=11.199754357952495\n",
      "Stochastic Gradient Descent(28/49): loss=40.42963207978308, w0=66.48270949351095, w1=11.557507299037946\n",
      "Stochastic Gradient Descent(29/49): loss=39.55802174650098, w0=66.72878082140501, w1=11.189911344759185\n",
      "Stochastic Gradient Descent(30/49): loss=34.34107178879237, w0=67.65669629231702, w1=11.003413960081534\n",
      "Stochastic Gradient Descent(31/49): loss=33.37560426526466, w0=67.94575772224425, w1=10.763727967343826\n",
      "Stochastic Gradient Descent(32/49): loss=32.4610836407088, w0=68.12334174693517, w1=10.756572451696828\n",
      "Stochastic Gradient Descent(33/49): loss=30.464998282145324, w0=68.55014734398023, w1=10.712977388121843\n",
      "Stochastic Gradient Descent(34/49): loss=25.887770731169788, w0=69.19704739092339, w1=11.425598478818461\n",
      "Stochastic Gradient Descent(35/49): loss=27.127769641991595, w0=68.74217734483442, w1=11.816768021807595\n",
      "Stochastic Gradient Descent(36/49): loss=24.52093457594693, w0=69.22470678432637, w1=12.171438409552344\n",
      "Stochastic Gradient Descent(37/49): loss=22.962854341815504, w0=69.60551567217851, w1=12.234886466699095\n",
      "Stochastic Gradient Descent(38/49): loss=22.72238135121023, w0=69.66121020305282, w1=12.264643733315213\n",
      "Stochastic Gradient Descent(39/49): loss=22.020921767472647, w0=69.8656564851508, w1=12.24802118205757\n",
      "Stochastic Gradient Descent(40/49): loss=20.299892260593367, w0=70.27540265735459, w1=12.633219819017505\n",
      "Stochastic Gradient Descent(41/49): loss=17.89944587102011, w0=71.09438486789796, w1=13.044795891701888\n",
      "Stochastic Gradient Descent(42/49): loss=18.223696437357173, w0=70.93387649996563, w1=13.154439680386995\n",
      "Stochastic Gradient Descent(43/49): loss=17.00963819016149, w0=71.51989678820003, w1=13.796469728658694\n",
      "Stochastic Gradient Descent(44/49): loss=17.027263638315866, w0=71.5064821193338, w1=13.776040301466553\n",
      "Stochastic Gradient Descent(45/49): loss=18.12568902064747, w0=70.98880421204935, w1=13.887185474102308\n",
      "Stochastic Gradient Descent(46/49): loss=17.971433097930998, w0=71.07223632976661, w1=13.964689992746795\n",
      "Stochastic Gradient Descent(47/49): loss=17.80295885900881, w0=71.25179050886824, w1=14.29447678416533\n",
      "Stochastic Gradient Descent(48/49): loss=18.720920583301258, w0=71.0241624587748, w1=14.711888170996016\n",
      "Stochastic Gradient Descent(49/49): loss=17.002953192351352, w0=71.52862334883812, w1=13.823007223497958\n",
      "SGD: execution time=0.152 seconds\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "import datetime\n",
    "from implementations import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "\n",
    "\n",
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.01\n",
    "batch_size = 128 # do not pass  batch_size in arg as in the definition\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([71, 13])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = least_squares_SGD(\n",
    "\t\ty, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function run_test at 0x00000000047792F0>\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
