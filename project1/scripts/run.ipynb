{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/49): loss=324.4744177219349, w0=50.22591312086205, w1=4.203720522471046\n",
      "Stochastic Gradient Descent(1/49): loss=53.13038735045485, w0=65.87023503239065, w1=8.965526933822577\n",
      "Stochastic Gradient Descent(2/49): loss=18.77615482184298, w0=71.39157123532132, w1=11.701624848241986\n",
      "Stochastic Gradient Descent(3/49): loss=15.712168834975932, w0=72.71188653565868, w1=12.919536883852215\n",
      "Stochastic Gradient Descent(4/49): loss=15.637904445025372, w0=73.07896373551773, w1=14.156340912085351\n",
      "Stochastic Gradient Descent(5/49): loss=15.41632624916501, w0=73.06481189750785, w1=13.571283831410744\n",
      "Stochastic Gradient Descent(6/49): loss=15.423932062669191, w0=73.47325050987826, w1=13.270118365465434\n",
      "Stochastic Gradient Descent(7/49): loss=15.49701600105532, w0=73.33877641145551, w1=13.010410457139313\n",
      "Stochastic Gradient Descent(8/49): loss=15.524387722725061, w0=73.19583550184103, w1=12.962625647053743\n",
      "Stochastic Gradient Descent(9/49): loss=15.49824626168677, w0=73.3112125932092, w1=13.0059848538055\n",
      "Stochastic Gradient Descent(10/49): loss=15.390986522734453, w0=73.30802752939263, w1=13.579704144195604\n",
      "Stochastic Gradient Descent(11/49): loss=15.440252330060158, w0=73.10701047088122, w1=13.208063761226354\n",
      "Stochastic Gradient Descent(12/49): loss=15.409056722695814, w0=73.42110380491896, w1=13.653385970007674\n",
      "Stochastic Gradient Descent(13/49): loss=15.592744523864853, w0=73.02355297580885, w1=14.063333801707644\n",
      "Stochastic Gradient Descent(14/49): loss=15.538667745116603, w0=73.05252685959165, w1=12.98243169160029\n",
      "Stochastic Gradient Descent(15/49): loss=15.53858440063689, w0=73.84188242515027, w1=13.551353473446285\n",
      "Stochastic Gradient Descent(16/49): loss=15.530620521213713, w0=73.81959076555337, w1=13.59433204894532\n",
      "Stochastic Gradient Descent(17/49): loss=15.586925914562865, w0=72.99244017451018, w1=12.921873032385662\n",
      "Stochastic Gradient Descent(18/49): loss=15.632377656375098, w0=72.71462567283892, w1=13.082981621981006\n",
      "Stochastic Gradient Descent(19/49): loss=15.41239579726186, w0=73.0663429413128, w1=13.444732041140933\n",
      "Stochastic Gradient Descent(20/49): loss=15.712349059679944, w0=74.04269344762803, w1=13.783461844981453\n",
      "Stochastic Gradient Descent(21/49): loss=15.461881604541542, w0=73.41204360371371, w1=13.108182141933797\n",
      "Stochastic Gradient Descent(22/49): loss=15.590871973080194, w0=73.20059136076318, w1=12.846263463178154\n",
      "Stochastic Gradient Descent(23/49): loss=15.45406503235249, w0=73.64952442397355, w1=13.380207437086725\n",
      "Stochastic Gradient Descent(24/49): loss=15.567804571137817, w0=73.64813369659399, w1=12.991483513166298\n",
      "Stochastic Gradient Descent(25/49): loss=15.552872958461899, w0=73.56037035818731, w1=12.966901112175162\n",
      "Stochastic Gradient Descent(26/49): loss=15.538695924497635, w0=73.84671774654244, w1=13.485454931705018\n",
      "Stochastic Gradient Descent(27/49): loss=15.408574385099067, w0=73.50693082550877, w1=13.47918927960689\n",
      "Stochastic Gradient Descent(28/49): loss=15.635871810971548, w0=72.84595265398036, w1=14.026787701679943\n",
      "Stochastic Gradient Descent(29/49): loss=15.535625630118162, w0=73.74444284493457, w1=13.790367376610515\n",
      "Stochastic Gradient Descent(30/49): loss=15.49655431130659, w0=73.75521464558764, w1=13.387289591366153\n",
      "Stochastic Gradient Descent(31/49): loss=15.570361305141443, w0=72.8970739443152, w1=13.939558595843296\n",
      "Stochastic Gradient Descent(32/49): loss=15.478473933813424, w0=73.23571858375266, w1=13.053350508851029\n",
      "Stochastic Gradient Descent(33/49): loss=15.438595261317035, w0=73.40388420240936, w1=13.174224138192704\n",
      "Stochastic Gradient Descent(34/49): loss=15.5823390723938, w0=72.79630557264404, w1=13.098555910370692\n",
      "Stochastic Gradient Descent(35/49): loss=15.433967958412461, w0=73.5968060436328, w1=13.54620631857262\n",
      "Stochastic Gradient Descent(36/49): loss=15.499192785080504, w0=72.86769943057514, w1=13.69171279352773\n",
      "Stochastic Gradient Descent(37/49): loss=15.408233497175306, w0=73.08262777472918, w1=13.472929650208352\n",
      "Stochastic Gradient Descent(38/49): loss=15.395821281385999, w0=73.43424241969946, w1=13.493016777113672\n",
      "Stochastic Gradient Descent(39/49): loss=15.52587580426305, w0=73.6690456973644, w1=13.106539440089394\n",
      "Stochastic Gradient Descent(40/49): loss=15.456016736759555, w0=73.60364671474241, w1=13.269169474427303\n",
      "Stochastic Gradient Descent(41/49): loss=15.458779738884376, w0=73.66085540513248, w1=13.3741490992481\n",
      "Stochastic Gradient Descent(42/49): loss=15.45508298997139, w0=73.62331642958783, w1=13.652598389963314\n",
      "Stochastic Gradient Descent(43/49): loss=15.455257603185428, w0=73.08375748696027, w1=13.78723535329914\n",
      "Stochastic Gradient Descent(44/49): loss=15.389467297431128, w0=73.34168972189812, w1=13.549548687388784\n",
      "Stochastic Gradient Descent(45/49): loss=15.484835569891727, w0=72.86897828705756, w1=13.611311223608729\n",
      "Stochastic Gradient Descent(46/49): loss=15.45586699120935, w0=73.02764206378531, w1=13.74249226528098\n",
      "Stochastic Gradient Descent(47/49): loss=15.685222266474794, w0=73.40150739343439, w1=14.245933146575032\n",
      "Stochastic Gradient Descent(48/49): loss=15.427139542641372, w0=73.12851762407307, w1=13.714541605561521\n",
      "Stochastic Gradient Descent(49/49): loss=15.430205989114588, w0=73.0523381512157, w1=13.653705203688725\n",
      "SGD: execution time=0.140 seconds\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "from test_sgd import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "\n",
    "\n",
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "batch_size = 128 # do not pass  batch_size in arg as in the definition\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = least_squares_SGD(\n",
    "\t\ty, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function run_test at 0x00000000047792F0>\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
