{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(0/49): loss=23.15754282638566, w0=50.13704689928802, w1=7.92470357783445\n",
      "Stochastic Gradient Descent(1/49): loss=7.1150864366534305, w0=66.83285313795955, w1=13.16204661643595\n",
      "Stochastic Gradient Descent(2/49): loss=4.565773327109816, w0=71.85839493137982, w1=13.269050021065347\n",
      "Stochastic Gradient Descent(3/49): loss=4.448768757926642, w0=73.25115225388356, w1=13.003593252409551\n",
      "Stochastic Gradient Descent(4/49): loss=4.425702253570969, w0=73.1654358734502, w1=13.61256799621227\n",
      "Stochastic Gradient Descent(5/49): loss=4.431145956997326, w0=73.17553862866521, w1=13.817281015240376\n",
      "Stochastic Gradient Descent(6/49): loss=4.440696021065981, w0=73.70389688081099, w1=13.563781975563673\n",
      "Stochastic Gradient Descent(7/49): loss=4.42564419521775, w0=73.12263192252995, w1=13.527494337641745\n",
      "Stochastic Gradient Descent(8/49): loss=4.435802456398869, w0=73.44282264165376, w1=13.218562074537578\n",
      "Stochastic Gradient Descent(9/49): loss=4.432834718711944, w0=73.16146406649223, w1=13.852389938485004\n",
      "Stochastic Gradient Descent(10/49): loss=4.434894740165402, w0=73.50437591369301, w1=13.278147631634427\n",
      "Stochastic Gradient Descent(11/49): loss=4.44751060515482, w0=73.78599596731254, w1=13.613371972855674\n",
      "Stochastic Gradient Descent(12/49): loss=4.435874080874657, w0=73.50498348061087, w1=13.809600404728412\n",
      "Stochastic Gradient Descent(13/49): loss=4.438229201963563, w0=73.32294273913818, w1=13.948079184836663\n",
      "Stochastic Gradient Descent(14/49): loss=4.425100242163093, w0=73.15901589679109, w1=13.527973435707155\n",
      "Stochastic Gradient Descent(15/49): loss=4.425505770947187, w0=73.27637991496252, w1=13.630408413258175\n",
      "Stochastic Gradient Descent(16/49): loss=4.449691302738669, w0=73.6895979073712, w1=13.18514072355471\n",
      "Stochastic Gradient Descent(17/49): loss=4.434435960221897, w0=73.60270454912306, w1=13.585701008019207\n",
      "Stochastic Gradient Descent(18/49): loss=4.432461791786293, w0=73.54633996573075, w1=13.409305556474711\n",
      "Stochastic Gradient Descent(19/49): loss=4.445088352966076, w0=73.74751637602151, w1=13.652590621896021\n",
      "Stochastic Gradient Descent(20/49): loss=4.4257237498170605, w0=73.27402637339537, w1=13.414257810910957\n",
      "Stochastic Gradient Descent(21/49): loss=4.463219276673248, w0=73.92737157239056, w1=13.35347079731644\n",
      "Stochastic Gradient Descent(22/49): loss=4.460324716834054, w0=73.92464229510095, w1=13.5905532567098\n",
      "Stochastic Gradient Descent(23/49): loss=4.465061307450321, w0=73.30647937742377, w1=14.25786646516359\n",
      "Stochastic Gradient Descent(24/49): loss=4.463451266249417, w0=72.48799946658528, w1=13.521828812405063\n",
      "Stochastic Gradient Descent(25/49): loss=4.427907052876632, w0=73.24098153358351, w1=13.331463313655147\n",
      "Stochastic Gradient Descent(26/49): loss=4.4281362944670875, w0=73.15776884180801, w1=13.343385490059521\n",
      "Stochastic Gradient Descent(27/49): loss=4.46330879897633, w0=73.29228242582255, w1=12.864756287577631\n",
      "Stochastic Gradient Descent(28/49): loss=4.428128692980641, w0=73.45732027734593, w1=13.578982430893674\n",
      "Stochastic Gradient Descent(29/49): loss=4.4486808959063, w0=73.80313676133319, w1=13.489876984365916\n",
      "Stochastic Gradient Descent(30/49): loss=4.436776985005656, w0=73.52048334742534, w1=13.252594493693698\n",
      "Stochastic Gradient Descent(31/49): loss=4.42538224450959, w0=73.14385943488635, w1=13.554374585309557\n",
      "Stochastic Gradient Descent(32/49): loss=4.496468569424709, w0=73.92892540574782, w1=12.878484497883429\n",
      "Stochastic Gradient Descent(33/49): loss=4.449415071292971, w0=73.490925123378, w1=13.043732765339392\n",
      "Stochastic Gradient Descent(34/49): loss=4.4761835236352505, w0=72.95137929949585, w1=12.805347926801485\n",
      "Stochastic Gradient Descent(35/49): loss=4.467168937098719, w0=73.63256648253002, w1=12.927745678233888\n",
      "Stochastic Gradient Descent(36/49): loss=4.441998802685786, w0=72.89965118572191, w1=13.203807784325475\n",
      "Stochastic Gradient Descent(37/49): loss=4.426478592371597, w0=73.37959446414075, w1=13.609675957942208\n",
      "Stochastic Gradient Descent(38/49): loss=4.484045391180776, w0=73.9949922137595, w1=13.974986627769832\n",
      "Stochastic Gradient Descent(39/49): loss=4.469820936719581, w0=73.81593707351824, w1=14.0221721023576\n",
      "Stochastic Gradient Descent(40/49): loss=4.5012520138620085, w0=74.05028130868054, w1=14.10938422388909\n",
      "Stochastic Gradient Descent(41/49): loss=4.440349323650448, w0=73.4703301281882, w1=13.157383344595225\n",
      "Stochastic Gradient Descent(42/49): loss=4.433984197465771, w0=73.59469490468726, w1=13.58359317015411\n",
      "Stochastic Gradient Descent(43/49): loss=4.436287293990002, w0=73.63360041929549, w1=13.592253564744503\n",
      "Stochastic Gradient Descent(44/49): loss=4.431421434711565, w0=73.53086246624096, w1=13.42738793800206\n",
      "Stochastic Gradient Descent(45/49): loss=4.438598617459911, w0=73.02858281624815, w1=13.914504715286824\n",
      "Stochastic Gradient Descent(46/49): loss=4.426843397304023, w0=73.06454591538203, w1=13.53835654964091\n",
      "Stochastic Gradient Descent(47/49): loss=4.500289734905828, w0=72.25637782401411, w1=13.884662484367574\n",
      "Stochastic Gradient Descent(48/49): loss=4.444990813452371, w0=72.70501645449697, w1=13.45616833811623\n",
      "Stochastic Gradient Descent(49/49): loss=4.43498988002755, w0=73.42555182415045, w1=13.849807829338928\n",
      "SGD: execution time=0.290 seconds\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "from test_sgd import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "\n",
    "\n",
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "batch_size = 128 # do not pass  batch_size in arg as in the definition\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = least_squares_SGD(\n",
    "\t\ty, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function run_test at 0x00000000047792F0>\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
