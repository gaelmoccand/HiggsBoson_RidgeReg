\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment
\usepackage{amsmath}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Some tools
% Includes a figure
% The first parameter is the label, which is also the name of the figure
%   with or without the extension (e.g., .eps, .fig, .png, .gif, etc.)
%   IF NO EXTENSION IS GIVEN, LaTeX will look for the most appropriate one.
%   This means that if a DVI (or PS) is being produced, it will look for
%   an eps. If a PDF is being produced, it will look for nearly anything
%   else (gif, jpg, png, et cetera). Because of this, when I generate figures
%   I typically generate an eps and a png to allow me the most flexibility
%   when rendering my document.
% The second parameter is the width of the figure normalized to column width
%   (e.g. 0.5 for half a column, 0.75 for 75% of the column)
% The third parameter is the caption.
\newcommand{\scalefig}[4]{
  \begin{figure}[ht!]
    % Requires \usepackage{graphicx}
    \centering
    \includegraphics[width=#2\columnwidth]{../pics/#1}
 \caption{#3}
    \label{#4}
  \end{figure}}
  
  \newcommand{\doublefig}[5]{
  \begin{figure}[ht!]
    % Requires \usepackage{graphicx}
    \centering
    \includegraphics[width=#3\columnwidth]{../pics/#1}\hfill
    \includegraphics[width=#3\columnwidth]{../pics/#2}
 \caption{#4}
    \label{#5}
  \end{figure}}

% \mathbf
\newcommand{\m}[1]{\mathbf{#1}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\xt}{\mathbf{x}^T}
\newcommand{\yy}{\mathbf{y}}
\newcommand{\ww}{\mathbf{w}}
\newcommand{\XX}{\mathbf{X}}
 
\newcommand{\Lagr}{\mathcal{L}}
\newcommand{\Lmse}{\Lagr_{MSE}}

% argmin
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{argmax}}}
\newcommand{\mmin}[1]{\underset{#1}{\operatorname{min}}}
\newcommand{\mmax}[1]{\underset{#1}{\operatorname{max}}}

\newcommand*\colvec[3][]{
    \begin{pmatrix}\ifx\relax#1\relax\else#1\\\fi#2\\#3\end{pmatrix}
}


\begin{document}
\title{Project 1: Higgs Boson}

\author{
  Gael Moccand, Pascal Bienz\\
  gael.moccand@gmail.com, pascal.bienz@gmail.com\\
  \textit{Machine Learning Course, EPFL}
}

\maketitle

\begin{abstract}
The abstract should really be written last, along with the title of
the paper. The four points that should be covered:
\begin{enumerate}
\item State the problem.
\item Say why it is an interesting problem.
\item Say what your solution achieves.
\item Say what follows from your solution.
\end{enumerate}
\end{abstract}

\section{Introduction}
The Higgs boson is an elementary particle discovered at the Large Hadron Collider at CERN in 2013. In order to produce it, physicists accelerate protons and make them collide at high speeds. In some rare cases, the collision generates a Higgs boson. A major problem that arises when scientists want to observe the particle is that its life is very short. Indeed, a Higgs boson quickly decays into other particles. For that reason, it is observed indirectly by looking at the outputs of the decay. However, this process can become tricky because a Higgs boson's decay signature can be very much alike another particle's signature.\\
In this paper, a machine learning method that efficiently estimates the likelihood that a given measurement is due to a Higgs boson or some other particles is presented... ADD SOME DETAILS ABOUT THE METHOD HERE.\\
Two specific data sets are used to optimize the method. The first set $S_{t}$ is called the training set and contains $N=250000$ observations. It is used to develop the model. The second set $S_{v}$ is the validation set and has 568238 events. These data are used to validate the model and make sure that we the model does not overfit the data of $S_t$. In both sets, the events are characterized by 30 features. Among them, 13 are "raw" quantities about the bunch collision as measured by the detector and 17 are quantities computed from the raw features, which were selected by the physicists. Finally, let's point out that in some cases the variables of some entries are not available. In order to handle the missing data, it is primordial to apply a preprocessing stage.

\section{Methodology}
To predict the nature of the measurement, we need to find a function that best approximates the output $\yy$ with the given inputs $\xx$
$$
y_n \approx f(\mathbf{x_n}) \ \forall n.
$$
A common choice is to use a linear regression, i.e.
$$
f(\mathbf{x_n}) = w_0 +  \xx_n^T\begin{pmatrix} w_1 \\ \vdots\\ w_D \end{pmatrix} =: \tilde{\xx}_n^T \ww
$$
where $\ww = (w_0 \ldots w_D)$ are the parameters of the models. Note that we add a tilde over the input vector to indicate that it contains an offset term.\\
A cost function is needed to estimate how well the model does. Again, a common choice is to use the Mean Square Error (MSE):
$$
\Lmse(\ww) := \frac 1N \sum_{n=1}^N \left( y_n -f(\xx_n) \right)^2
$$
As a starting point, it has been decided to chose a simple least squares regression  to compute $\ww$ directly. The parameters are given by the following expression:
$$
\ww^* = (\XX^T\XX)^{-1}\XX^T\yy
$$
which leads to a prediction for an unseen datapoint $\xx_m$ given by
$$
\hat{y}_m := \xx_m^T\ww^*=\xx_m^T(\XX^T\XX)^{-1}\XX^T\yy,
$$
where $\yy = \begin{bmatrix}y_1 \\ y_2 \\ \vdots \\y_N \end{bmatrix}$ and $\XX = \begin{bmatrix}x_{11} & x_{12} & \hdots & x_{1D} \\ x_{21} & x_{22} & \hdots & x_{2D} \\ \vdots & \vdots & \ddots & \vdots \\ x_{N1} & x_{N2} & \hdots & x_{ND} \end{bmatrix}$.
Linear models are inherently not very rich. A way to increase their representational power  one can boost the input by adding a polynomial basis of arbitrary degree $M$:
$$
\phi(\xx_n) = \begin{bmatrix} 1 & x_{n1} & x_{n1}^2 & \hdots & x_{n1}^M \\ 1& x_{n2} & x_{n2}^2 & \hdots & x_{n2}^M \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1& x_{nD} & x_{nD}^2 & \hdots & x_{nD}^M \end{bmatrix}.
$$
We then fit a linear model to the extended feature vector $\phi(\xx_n)$:
$$
y_n \approx \phi(\xx_n)^T\ww.
$$

%As it can be seen on the figure cross_validation_least_poly, it is ovious than the smalest error is obtained by chosing a degree of 2 for the least square. However not every polynomial basis could be used because
%some basis leads to a unsolvable solution, that's the reason why some degree point are missing on the figure.
%The least square with a polynomial basis of 2 gives a score kaggle of 0.7718

%poly


\section{Results}
In order to measure the quality of a method, the predictions obtained are sent to the predictive modelling competitions platform \textit{Kaggle} which returns a score between 0 and 1.\\
\section{Discussion}
MSE is not a good cost function
when outliers are present.
\section{Summary}

%
%\begin{figure}[tbp]
%  \centering
%  \includegraphics[width=\columnwidth]{denoised_signal_1d}
%  \caption{Signal compression and denoising using the Fourier basis.}
%  \vspace{-3mm}
%  \label{fig:denoise-fourier}
%\end{figure}
%\begin{figure}[htbp]
%  \centering
%  \includegraphics[width=\columnwidth]{local_wdenoised_1d}
%  \vspace{-3mm}
%  \caption{Signal compression and denoising using the Daubechies wavelet basis.}
%  \label{fig:denoise-wavelet}
%\end{figure}


%\begin{table*}[htbp]
%  \centering
%  \begin{tabular}[c]{|l||l|l|l|}
%    \hline
%    Basis&Support&Suitable signals&Unsuitable signals\\
%    \hline
%    Fourier&global&sine like&localized\\
%    wavelet&local&localized&sine like\\
%    \hline
%  \end{tabular}
%  \caption{Characteristics of Fourier and wavelet basis.}
%  \label{tab:fourier-wavelet}
%\end{table*}

%\section*{Acknowledgements}
%The author thanks Christian Sigg for his careful reading and helpful
%suggestions.

\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
