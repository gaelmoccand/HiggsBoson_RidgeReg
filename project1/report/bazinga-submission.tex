\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment
\usepackage{amsmath}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Some tools
% Includes a figure
% The first parameter is the label, which is also the name of the figure
%   with or without the extension (e.g., .eps, .fig, .png, .gif, etc.)
%   IF NO EXTENSION IS GIVEN, LaTeX will look for the most appropriate one.
%   This means that if a DVI (or PS) is being produced, it will look for
%   an eps. If a PDF is being produced, it will look for nearly anything
%   else (gif, jpg, png, et cetera). Because of this, when I generate figures
%   I typically generate an eps and a png to allow me the most flexibility
%   when rendering my document.
% The second parameter is the width of the figure normalized to column width
%   (e.g. 0.5 for half a column, 0.75 for 75% of the column)
% The third parameter is the caption.
\newcommand{\scalefig}[4]{
  \begin{figure}[ht!]
    % Requires \usepackage{graphicx}
    \centering
    \includegraphics[width=#2\columnwidth]{pics/#1}
 \caption{#3}
    \label{#4}
  \end{figure}}
  
  \newcommand{\doublefig}[5]{
  \begin{figure}[ht!]
    % Requires \usepackage{graphicx}
    \centering
    \includegraphics[width=#3\columnwidth]{../pics/#1}\hfill
    \includegraphics[width=#3\columnwidth]{../pics/#2}
 \caption{#4}
    \label{#5}
  \end{figure}}

% \mathbf
\newcommand{\m}[1]{\mathbf{#1}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\xt}{\mathbf{x}^T}
\newcommand{\yy}{\mathbf{y}}
\newcommand{\ww}{\mathbf{w}}
\newcommand{\XX}{\mathbf{X}}
 
\newcommand{\Lagr}{\mathcal{L}}
\newcommand{\Lmse}{\Lagr_{MSE}}

% argmin
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}}
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{argmax}}}
\newcommand{\mmin}[1]{\underset{#1}{\operatorname{min}}}
\newcommand{\mmax}[1]{\underset{#1}{\operatorname{max}}}

\newcommand*\colvec[3][]{
    \begin{pmatrix}\ifx\relax#1\relax\else#1\\\fi#2\\#3\end{pmatrix}
}


\begin{document}
\title{Project 1: Higgs Boson}

\author{
  Gael Moccand, Pascal Bienz\\
  gael.moccand@gmail.com, pascal.bienz@gmail.com\\
  \textit{Machine Learning Course, EPFL}
}

\maketitle

\begin{abstract}
The abstract should really be written last, along with the title of
the paper. The four points that should be covered:
\begin{enumerate}
\item State the problem.
\item Say why it is an interesting problem.
\item Say what your solution achieves.
\item Say what follows from your solution.
\end{enumerate}
\end{abstract}

\section{Introduction}
The Higgs boson is an elementary particle discovered at the Large Hadron Collider at CERN in 2013. In order to produce it, physicists accelerate protons and make them collide at high speeds. In some rare cases, the collision generates a Higgs boson. A major problem that arises when scientists want to observe the particle is that its life is very short. Indeed, a Higgs boson quickly decays into other particles. For that reason, it is observed indirectly by looking at the outputs of the decay. However, this process can become tricky because a Higgs boson's decay signature can be very much alike another particle's signature.\\
In this paper, a machine learning method that efficiently estimates the likelihood that a given measurement is due to a Higgs boson or some other particles is presented... ADD SOME DETAILS ABOUT THE METHOD HERE.\\
Two specific data sets are used to optimize the method. The first set $S_{t}$ is called the training set and contains $N=250000$ observations. It is used to develop the model. The second set $S_{v}$ is the validation set and has 568238 events. These data are used to validate the model and make sure that we the model does not overfit the data of $S_t$. In both sets, the events are characterized by 30 features. Among them, 13 are "raw" quantities about the bunch collision as measured by the detector and 17 are "derived" quantities computed from the raw features, which were selected by the physicists. Finally, let's point out that in some cases the variables of some entries are not available. In order to handle the missing data, it is primordial to apply a preprocessing stage.

\section{Methodology}
Before digging into prediction methods, it is beneficial to handle the data. The preprocessing is key and impacts the prediction. For instance, a scatter plot matrix of each features can be useful to select or drop some features. In our case, a basic standardization by subtracting the mean and dividing by the standard deviation has been applied to the input data. Furthermore, the missing value have been replaced by the mean of the corresponding feature, which is better than a value which has no signification.\\
To predict the nature of the measurement, we need to find a function that best approximates the output $\yy$ with the given inputs $\xx$
$$
y_n \approx f(\mathbf{x_n}) \ \forall n.
$$
A common choice is to use a linear regression, i.e.
$$
f(\mathbf{x_n}) = w_0 +  \xx_n^T\begin{pmatrix} w_1 \\ \vdots\\ w_D \end{pmatrix} =: \tilde{\xx}_n^T \ww
$$
where $\ww = (w_0 \ldots w_D)$ are the parameters of the models. Note that we add a tilde over the input vector to indicate that it contains an offset term.\\
A cost function is needed to estimate how well the model does. Again, a common choice is to use the Mean Square Error (MSE):
$$
\Lmse(\ww) := \frac 1N \sum_{n=1}^N \left( y_n -f(\xx_n) \right)^2
$$
As a starting point, it has been decided to chose a simple least squares regression  to compute $\ww$ directly. The parameters are given by the following expression:
$$
\ww^* = (\XX^T\XX)^{-1}\XX^T\yy
$$
which leads to a prediction for an unseen data point $\xx_m$ given by
$$
\hat{y}_m := \xx_m^T\ww^*=\xx_m^T(\XX^T\XX)^{-1}\XX^T\yy,
$$
where $\yy = \begin{bmatrix}y_1 \\ y_2 \\ \vdots \\y_N \end{bmatrix}$ and $\XX = \begin{bmatrix}x_{11} & x_{12} & \hdots & x_{1D} \\ x_{21} & x_{22} & \hdots & x_{2D} \\ \vdots & \vdots & \ddots & \vdots \\ x_{N1} & x_{N2} & \hdots & x_{ND} \end{bmatrix}$.
Linear models are inherently not very rich. A way to increase their representational power  one can boost the input by adding a polynomial basis of arbitrary degree $M$:
$$
\phi(\xx_n) = \begin{bmatrix} 1 & x_{n1} & x_{n1}^2 & \hdots & x_{n1}^M \\ 1& x_{n2} & x_{n2}^2 & \hdots & x_{n2}^M \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1& x_{nD} & x_{nD}^2 & \hdots & x_{nD}^M \end{bmatrix}.
$$
We then fit a linear model to the extended feature vector $\phi(\xx_n)$:
$$
y_n \approx \phi(\xx_n)^T\ww.
$$
Unfortunately, this tuning has a negative effect: overfitting. Regularization is a
way to mitigate this undesirable behavior by penalizing the model with a parameter $\Omega(\ww)$. The optimization problem becomes
$$
\argmin{\ww} \Lagr(\ww) + \Omega(\ww).
$$ 
The Ridge regularization $\Omega(\ww) = \lambda || \ww ||_2^2$ is a natural choice. It penalizes the large model weights $w_i$.

%As it can be seen on the figure cross_validation_least_poly, it is ovious than the smalest error is obtained by chosing a degree of 2 for the least square. However not every polynomial basis could be used because
%some basis leads to a unsolvable solution, that's the reason why some degree point are missing on the figure.
%The least square with a polynomial basis of 2 gives a score kaggle of 0.7718

%poly


\section{Results}
In order to measure the quality of a method, the predictions obtained are sent to the predictive modelling competitions platform \textit{Kaggle} which returns a score between 0 and 1. \\
As a baseline, a simple least squares together with a polynomial basis with different degrees has been used. As it can be seen on Fig.\ref{fig:cross_validation_least_poly}, it is obvious that the smallest error is obtained by choosing a degree of. However not every polynomial basis could be used because some basis lead to a unsolvable solution, that's the reason why some degree point are missing on the figure.
\scalefig{cross_validation_least_poly}{1}{MSE w.r.t polynomial degree for direct least squares method}{fig:cross_validation_least_poly}

It seems that selecting raw or the derived features only gives a lower score than taking into consideration both types, even if there is information redundancy by taking both types. 
Moreover, there is a feature which only has integer numbers. A possible improvement would be not to feed it to the polynomial basis and use this feature directly. Unfortunately, this trick has not shown much improvement on the final score.


\section{Discussion}
MSE is not a good cost function when outliers are present.

Another improvement would be also to weight the features independently because fot the moment each feature are equally weighted. 
\section{Summary}

%
%\begin{figure}[tbp]
%  \centering
%  \includegraphics[width=\columnwidth]{denoised_signal_1d}
%  \caption{Signal compression and denoising using the Fourier basis.}
%  \vspace{-3mm}
%  \label{fig:denoise-fourier}
%\end{figure}
%\begin{figure}[htbp]
%  \centering
%  \includegraphics[width=\columnwidth]{local_wdenoised_1d}
%  \vspace{-3mm}
%  \caption{Signal compression and denoising using the Daubechies wavelet basis.}
%  \label{fig:denoise-wavelet}
%\end{figure}


%\begin{table*}[htbp]
%  \centering
%  \begin{tabular}[c]{|l||l|l|l|}
%    \hline
%    Basis&Support&Suitable signals&Unsuitable signals\\
%    \hline
%    Fourier&global&sine like&localized\\
%    wavelet&local&localized&sine like\\
%    \hline
%  \end{tabular}
%  \caption{Characteristics of Fourier and wavelet basis.}
%  \label{tab:fourier-wavelet}
%\end{table*}

%\section*{Acknowledgements}
%The author thanks Christian Sigg for his careful reading and helpful
%suggestions.

\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}
