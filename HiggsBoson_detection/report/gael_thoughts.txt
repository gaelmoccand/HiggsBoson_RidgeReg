Preprocessing:

The preprocessing is key and impacts the prediction. A scatterplot matrix of each features might also be useful to select or drop some features. Due to the lack of time this analysis 
has not been used so far. A basic standardization has been appliead of the x samples by substraction the mean and divided by the standard deviation. The value -999 need to be taken into
account as well. Our solution was to replace theses values by the mean computed on the same column (dimension) which is better than a value which has no signification. It seems that selecting
only the primitives feaures or the derivated ones give a lower score than taking into consideration both types, even if there should have information redundency by taking both types.
Moreover one feature has only integer numbers (PRI_jet_num), one possible improvment was to not feed this feature to the polynomial basis and use this feature directly. However this trick
has not shown much improvment on the final score.
Another improvement would be also to weight the features independently because fot the moment each feature are equally weighted.


Methodology:

The methodolgy is based on a 4 folds cross validation on the train.cvs data set. This solution is used to not overfit with the local data set and resulting by degrading the score of the test data set (online).

Hyperparameter:
In addition of the preprocessing and the type of regression(ridge,SGD,etc) used to find the best w vector, the hyperparameters have a big impact on the prediction. During this project theses
hyperparameters are the lambda (explain the influence of it ) and the degree of the polynomial basis used for the X features. The risk with a high degree of polynomial is the overfitting. For this reason
the need of cross-validation is required.

Baseline:
As a starting point, it has been decided to chose a simple least squares to compute directly the w vector together with different degree polynomial basis.
As it can be seen on the figure cross_validation_least_poly, it is ovious than the smalest error is obtained by chosing a degree of 2 for the least square. However not every polynomial basis could be used because
some basis leads to a unsolvable solution, that's the reason why some degree point are missing on the figure.
The least square with a polynomial basis of 2 gives a score kaggle of 0.7718

Improvement

The second step is to used the ridge regression. It can be seen in the figure bias_variance_ridge_poly_0.01 that quickly when increasing the degree of polynomial basis of the ridge regression with a lambda
of 0.01. 